{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EIdT9iu_Z4Rb"
   },
   "source": [
    "# Lettuce Weight Prediction: a Regression Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AHp3M9ZmrIxj"
   },
   "source": [
    "In a *regression* problem, we aim to predict the output of a continuous value, like a temperature or a probability. Contrast this with a *classification* problem, where we aim to select a class from a list of classes.\n",
    "\n",
    "We will use the new `tf.keras` API, see [this guide](https://www.tensorflow.org/guide/keras) for details.\n",
    "\n",
    "It aims to predict the lettuce fresh weight at a certain date after planting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup your environnement\n",
    "\n",
    "### Requirements\n",
    "\n",
    "```sh\n",
    "python3 --version\n",
    "pip3 --version\n",
    "virtualenv --version\n",
    "````\n",
    "\n",
    "### Create a virtualenv and install Tensorflow\n",
    "```sh \n",
    "virtualenv --system-site-packages -p python3 ./venv\n",
    "source ./venv/bin/activate\n",
    "pip install --upgrade pip\n",
    "pip install --upgrade tensorflow=2.0\n",
    "```\n",
    "\n",
    "#### Check you setup is correct\n",
    "```sh\n",
    "python -c \"import tensorflow as tf;print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rRo8oNqZ-Rj"
   },
   "outputs": [],
   "source": [
    "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import distro\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "print(tf.__version__)\n",
    "tf.reduce_sum(tf.random.normal([1000, 1000]))\n",
    "\n",
    "print('Running Tensorflow on {} {}'.format(distro.name(), distro.version()), distro.codename())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F_72b0LCNbjx"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset is available from the Github repository [IoT Tensorflow Lite DevoxxBE](https://github.com/alexisduque/iot-tensorflow-lite-devoxxbe) along with this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gFh9ne3FZ-On"
   },
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nslsRLh7Zss4"
   },
   "source": [
    "Import it using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CiX2FI4gZtTt",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataset_path = './datasets/lg_plantings.pickle'\n",
    "raw_dataset = pd.read_pickle('./datasets/lg_plantings.pickle')\n",
    "dataset = raw_dataset.copy()\n",
    "dataset.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3MWuJTKEDM-f"
   },
   "source": [
    "### Clean the data\n",
    "\n",
    "The dataset contains a few unknown values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JEJHhN65a2VV"
   },
   "outputs": [],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9UPN0KBHa_WI"
   },
   "source": [
    "To keep this initial tutorial simple drop those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZUDosChC1UN"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XKitwaH4v8h"
   },
   "source": [
    "The `\"Irrigation\"` column is categorical, not numeric. So lets converts to a One Hot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gWNTD2QjBWFJ"
   },
   "outputs": [],
   "source": [
    "irrigation = dataset.pop('Irrigation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ulXz4J7PAUzk"
   },
   "outputs": [],
   "source": [
    "dataset['HPA'] = (irrigation == 1)*1.0\n",
    "dataset['Ebb&Flood'] = (irrigation == 2)*1.0\n",
    "dataset['Nebu'] = (irrigation == 3)*1.0\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cuym4yvk76vU"
   },
   "source": [
    "### Split the data into train and test\n",
    "\n",
    "Now split the dataset into a training set and a test set.\n",
    "\n",
    "We will use the test set in the final evaluation of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qn-IGhUE7_1H"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8,random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4ubs136WLNp"
   },
   "source": [
    "### Inspect the data\n",
    "\n",
    "Have a quick look at the joint distribution of a few pairs of columns from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oRKO_x8gWKv-"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(train_dataset[[\"Weight\", \"CumCO2\", \"CumLight\", \"DoP\"]], diag_kind=\"kde\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gavKO_6DWRMP"
   },
   "source": [
    "Also look at the overall statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yi2FzC3T21jR"
   },
   "outputs": [],
   "source": [
    "train_stats = train_dataset.describe()\n",
    "train_stats.pop(\"Weight\")\n",
    "train_stats = train_stats.transpose()\n",
    "train_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Db7Auq1yXUvh"
   },
   "source": [
    "### Split features from labels\n",
    "\n",
    "Separate the target value, or \"label\", from the features. This label is the value that you will train the model to predict. Here ise the **weight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2sluJdCW7jN"
   },
   "outputs": [],
   "source": [
    "train_weights = train_dataset.pop('Weight')\n",
    "test_weights = test_dataset.pop('Weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mRklxK5s388r"
   },
   "source": [
    "### Normalize the data\n",
    "\n",
    "Look again at the `train_stats` block above and note how different the ranges of each feature are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ywmerQ6dSox"
   },
   "source": [
    "It is good practice to normalize features that use different scales and ranges. Although the model *might* converge without feature normalization, it makes training more difficult, and it makes the resulting model dependent on the choice of units used in the input.\n",
    "\n",
    "Note: Although we intentionally generate these statistics from only the training dataset, these statistics will also be used to normalize the test dataset. We need to do that to project the test dataset into the same distribution that the model has been trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlC5ooJrgjQF"
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return (x - train_stats['mean']) / train_stats['std']\n",
    "normed_train_data = norm(train_dataset)\n",
    "normed_test_data = norm(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BuiClDk45eS4"
   },
   "source": [
    "This normalized data is what we will use to train the model.\n",
    "\n",
    "Caution: The statistics used to normalize the inputs here (mean and standard deviation) need to be applied to any other data that is fed to the model, along with the one-hot encoding that we did earlier.  That includes the test set as well as live data when the model is used in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SmjdzxKzEu1-"
   },
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6SWtkIjhrZwa"
   },
   "source": [
    "### Build the model\n",
    "\n",
    "Let's build our model. Here, we'll use a `Sequential` model with two densely connected hidden layers, and an output layer that returns a single, continuous value. The model building steps are wrapped in a function, `build_model`, since we'll create a second model, later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c26juK7ZG8j-"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64, activation=tf.nn.relu, input_shape=[len(train_dataset.keys())]),\n",
    "        tf.keras.layers.Dense(64, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cGbPb-PHGbhs"
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sj49Og4YGULr"
   },
   "source": [
    "### Inspect the model\n",
    "\n",
    "Use the `.summary` method to print a simple description of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReAD0n6MsFK-"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, './figures/weight_reg_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt6W50qGsJAL",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Now try out the model. Take a batch of `10` examples from the training data and call `model.predict` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-d-gBaVtGTSC"
   },
   "outputs": [],
   "source": [
    "example_batch = normed_train_data[:10]\n",
    "example_result = model.predict(example_batch)\n",
    "example_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QlM8KrSOsaYo"
   },
   "source": [
    "It seems to be working, and it produces a result of the expected shape and type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0-qWCsh6DlyH"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Train the model using `early_stop` feature, and record the training and validation accuracy in the `history` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sD7qHCmNIOY0"
   },
   "outputs": [],
   "source": [
    "# Display training progress by printing a single dot for each completed epoch\n",
    "class print_dot(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if epoch % 200 == 0: print('')\n",
    "        if epoch % 2: print('.', end='')\n",
    "    \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir, histogram_freq=0)\n",
    "\n",
    "EPOCHS = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQm3pc0FYPQB"
   },
   "source": [
    "Visualize the model's training progress using the stats stored in the `history` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6XriGbVPh2t"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Abs Error [Weight]')\n",
    "    plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "             label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "             label = 'Val Error')\n",
    "    plt.ylim([0,120])\n",
    "    plt.legend()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [$Weight^2$]')\n",
    "    plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "             label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "             label = 'Val Error')\n",
    "    plt.ylim([0,14000])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fdMZuhUgzMZ4"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "# The patience parameter is the amount of epochs to check for improvement\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "history = model.fit(normed_train_data, train_weights, epochs=EPOCHS,\n",
    "                    validation_split = 0.2, verbose=0, callbacks=[early_stop, tensorboard_callback])\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Xj91b-dymEy"
   },
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3St8-DmrX8P4"
   },
   "source": [
    "The graph shows that on the validation set, the average error is usually around +/- 3 Weight.\n",
    "\n",
    "Let's see how well the model generalizes by using the **test** set, which we did not use when training the model.  This tells us how well we can expect the model to predict when we use it in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jl_yNr5n1kms"
   },
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(normed_test_data, test_weights, verbose=2)\n",
    "print(\"Testing set Mean Abs Error: {:5.2f}\".format(mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ft603OzXuEZC"
   },
   "source": [
    "### Make predictions\n",
    "\n",
    "Finally, predict Weight values using data in the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe7RXH3N3CWU",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_predictions = model.predict(normed_test_data).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe7RXH3N3CWU",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "def plot_predictions(true, inferences):\n",
    "    plt.scatter(true, inferences, marker='x')\n",
    "    plt.xlabel('True Values [Weight]')\n",
    "    plt.ylabel('Inferences [Weight]')\n",
    "    plt.axis('equal')\n",
    "    plt.axis('square')\n",
    "    plt.xlim([0,plt.xlim()[1]])\n",
    "    plt.ylim([0,plt.ylim()[1]])\n",
    "    plt.plot([-0, 250], [-0, 250], color='orange')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(test_weights, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "error = test_predictions - test_weights\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel(\"Prediction Error [Weight]\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export your Keras model to filesystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model_export_dir= \"./models/lg_weight/\"\n",
    "tf.saved_model.save(model,  model_export_dir)\n",
    "\n",
    "root_directory = pathlib.Path(model_export_dir)\n",
    "keras_model_size = sum(f.stat().st_size for f in root_directory.glob('**/*.pb') if f.is_file())\n",
    "print(\"Keras model is {} bytes\".format(keras_model_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Tensorflow Lite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using python API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# convert the model to the TensorFlow Lite format without quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# save the model to disk\n",
    "open('./models/lg_weight.tflite', \"wb\").write(tflite_model)\n",
    "  \n",
    "model_size = os.path.getsize('./models/lg_weight.tflite')\n",
    "print(\"TFLite model is {} bytes\".format(model_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using `tflite_converter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!tflite_convert \\\n",
    "  --saved_model_dir=$model_export_dir \\\n",
    "  --output_file='./models/lg_weight.tflite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the Model in a C Header File\n",
    "The next cell creates a constant byte array that contains the TFlite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!echo \"const unsigned char model[] = {\" > ./models/lg_weight_model.h\n",
    "!cat ./models/lg_weight.tflite | xxd -i >> ./models/lg_weight_model.h\n",
    "!echo \"};\"                              >> ./models/lg_weight_model.h\n",
    "\n",
    "model_h_size = os.path.getsize(\"./models/lg_weight_model.h\")\n",
    "# print(f\"Header file, lg_weight_model.h, is {model_h_size:,} bytes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight quantization\n",
    "The simplest form of post-training quantization quantizes only the weights from floating point to 8-bits of precision (also called \"hybrid\" quantization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# save the model to disk\n",
    "open('./models/lg_weight_quant.tflite', \"wb\").write(tflite_quant_model)\n",
    "  \n",
    "model_size = os.path.getsize('./models/lg_weight_quant.tflite')\n",
    "print(\"Model is %d bytes\" % model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vgGQuV-yqYZH"
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook introduced a the basics to handle a regression problem and apply them to lettuce weight prediction.\n",
    "\n",
    "* Mean Squared Error (MSE) is a common loss function used for regression problems (different loss functions are used for classification problems).\n",
    "* Similarly, evaluation metrics used for regression differ from classification. A common regression metric is Mean Absolute Error (MAE).\n",
    "* When numeric input data features have values with different ranges, each feature should be scaled independently to the same range.\n",
    "* If there is not much training data, one technique is to prefer a small network with few hidden layers to avoid overfitting.\n",
    "* Early stopping is a useful technique to prevent overfitting.\n",
    "\n",
    "It briefly introduced [Tensorboard](https://www.tensorflow.org/tensorboard/) to monitor the training process and vizualise the model graph and parameters.\n",
    "\n",
    "We then show some technics to convert a model after training to TF Lite/\n",
    "\n",
    "* Using the python API or the command line\n",
    "* From a saved model on the filesystem or a keras model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On your Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "converter_ffile = tf.lite.TFLiteConverter.from_saved_model(model_export_dir)\n",
    "converter_fkeras = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "\n",
    "tflite_model_ffile = converter_ffile.convert()\n",
    "tflite_model_fkeras = converter_fkeras.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Feed TFLite Interpreter with your model and allocate memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_ffile)\n",
    "# you must allocate tensors first\n",
    "interpreter.allocate_tensors()\n",
    "# obtaining the input-output shapes and types\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Iterate through the test dataset and run Tensorflow Lite inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "test_tflite_prediction = []\n",
    "for index,row in normed_test_data.iterrows():\n",
    "    input_tensor = np.array(row.values, 'f').reshape(input_details[0]['shape'])\n",
    "    # tensor index can be gotten from the 'index' field in get_input_details\n",
    "    interpreter.set_tensor(tensor_index=input_details[0]['index'], value=input_tensor)\n",
    "    # run inference\n",
    "    interpreter.invoke()\n",
    "    #é get result\n",
    "    inference = interpreter.get_tensor(output_details[0]['index'])\n",
    "    test_tflite_prediction.append(inference.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "test_tflite_prediction = np.array(test_tflite_prediction).flatten()\n",
    "plot_predictions(test_weights, test_tflite_prediction)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basic_regression.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
